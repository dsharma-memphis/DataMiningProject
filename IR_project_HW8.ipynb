{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6dM2o1HGya8KdVRPaGEaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsharma-memphis/DataMiningProject/blob/main/IR_project_HW8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, errno\n",
        "import math\n",
        "import time\n",
        "import operator\n",
        "import collections\n",
        "from collections import OrderedDict\n",
        "from collections import deque\n",
        "import queue\n",
        "import shutil\n",
        "import pickle\n",
        "import re\n",
        "import urllib\n",
        "from urllib.parse import urlsplit\n",
        "from urllib.parse import urljoin\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import datetime\n",
        "import numpy as np\n",
        "import docx2txt\n",
        "from pptx import Presentation\n",
        "import copy"
      ],
      "metadata": {
        "id": "qjPpH0LJ0QLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global Hashmaps, variables\n",
        "stopwords = {}\n",
        "term_doc_freq_vector = {}\n",
        "doc_term_freq_vector = {}\n",
        "doc_term_freq_vector_norm = {}\n",
        "page_doc_map = {}\n",
        "doc_page_map = {}\n",
        "page_ref_count = {}\n",
        "doc_count = 0\n",
        "link_queue = queue.Queue()\n",
        "last_doc_index = -1\n",
        "page_queued_map = {}\n",
        "start_time = time.time()\n",
        "total_number_docs = 0"
      ],
      "metadata": {
        "id": "2HO2FndA0SQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Global directory or file name\n",
        "\n",
        "crawled_web_dir = \"web_text_crawled\"\n",
        "crawled_web_dir_conv_need = \"web_docs_crawled\"\n",
        "crawled_web_dir_preprocessed = \"web_text_preprocessed\"\n",
        "output_web_dir = \"output\"\n",
        "\n",
        "stopword_path = \"english.stopwords.txt\"\n",
        "\n",
        "list_dir = [crawled_web_dir, crawled_web_dir_conv_need, crawled_web_dir_preprocessed]\n",
        "\n",
        "url = \"http://www.memphis.edu/\"\n",
        "#url = \"http://www.cs.memphis.edu/~vrus/teaching/ir-websearch/\"\n",
        "domain = \"memphis.edu\"\n",
        "total_number_docs = 10000"
      ],
      "metadata": {
        "id": "8vMkPNED0hQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File Operations\n",
        "# Create/Delete/ file/Directory\n",
        "\n",
        "# create one single directory\n",
        "def create_directory(directory):\n",
        "    try:\n",
        "        os.makedirs(directory)\n",
        "    except OSError as e:\n",
        "        if(e.errno != errno.EEXIST):\n",
        "            raise\n",
        "    pass\n",
        "\n",
        "\n",
        "# create a list of directories\n",
        "def create_directories(list_dir):\n",
        "    for dir_i in list_dir:\n",
        "        print(dir_i)\n",
        "        create_directory(dir_i)\n",
        "\n",
        "\n",
        "# delete one single directory\n",
        "def delete_directory(dir_name):\n",
        "    if(os.path.isdir(dir_name)):\n",
        "        try:\n",
        "            shutil.rmtree(dir_name)\n",
        "        except OSError as e:\n",
        "            if(e.errno != errno.EEXIST):\n",
        "                raise\n",
        "        pass\n",
        "\n",
        "\n",
        "# delete a list of directories\n",
        "def delete_directories(list_dir):\n",
        "    for dir_i in list_dir:\n",
        "        print(dir_i)\n",
        "        delete_directory(dir_i)\n",
        "\n",
        "\n",
        "#delete if file is empty\n",
        "def delete_file(path):\n",
        "    try:\n",
        "        os.remove(path)\n",
        "    except WindowsError:\n",
        "        print(\"failed deleting: \" + path)\n",
        "        pass\n",
        "\n",
        "\n",
        "#save text in given path with given file name\n",
        "def save_text(text, dir_path, file_name):\n",
        "    text_file = open(dir_path+\"\\\\\"+file_name, \"w\")\n",
        "    text_file.write(text)\n",
        "    text_file.close()\n",
        "\n",
        "\n",
        "#load stop words from given file path\n",
        "def load_stopwords(filepath):\n",
        "    with open(filepath, 'r') as content_file:\n",
        "        for line in content_file:\n",
        "            line = line.strip()\n",
        "            stopwords[line] = 1\n"
      ],
      "metadata": {
        "id": "6MHu1kPY0lO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save object in pickle\n",
        "def save_obj(obj, name, key_or_val, order):\n",
        "    filename = name + \".p\"\n",
        "\n",
        "    if(key_or_val == \"key\" and order == \"auto\"):\n",
        "        sorted_x = sorted(obj.items(), key=operator.itemgetter(0))\n",
        "    elif(key_or_val == \"key\" and order == \"reverse\"):\n",
        "        sorted_x = sorted(obj.items(), key=operator.itemgetter(0), reverse=True)\n",
        "    elif(key_or_val == \"value\" and order == \"auto\"):\n",
        "        sorted_x = sorted(obj.items(), key=operator.itemgetter(1))\n",
        "    elif(key_or_val == \"value\" and order == \"reverse\"):\n",
        "        sorted_x = sorted(obj.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    if (os.path.isfile(filename)):\n",
        "        os.remove(filename)\n",
        "\n",
        "    pickle.dump( obj, open( filename, \"wb\" ) )\n",
        "\n",
        "\n",
        "#save object\n",
        "def save_obj_without_sort(obj, name):\n",
        "    pickle.dump( obj, open( name + \".p\", \"wb\" ) )\n",
        "\n",
        "\n",
        "# save object in pickle\n",
        "def save_obj_no_sort(obj, name):\n",
        "    filename = name + \".p\"\n",
        "\n",
        "    if (os.path.isfile(filename)):\n",
        "        os.remove(filename)\n",
        "\n",
        "    pickle.dump( obj, open( filename, \"wb\" ) )\n",
        "\n",
        "\n",
        "#save current queue for future resume of crawling\n",
        "def save_obj_no_sort_w(queue1, filename):\n",
        "    link_list = []\n",
        "\n",
        "    new_queue = queue.Queue()\n",
        "    new_queue.queue = copy.deepcopy(queue1.queue)\n",
        "    if (os.path.isfile(filename)):\n",
        "        os.remove(filename)\n",
        "\n",
        "    while(new_queue.empty() == False):\n",
        "        link_list.append(new_queue.get())\n",
        "\n",
        "    filename = filename + \".p\"\n",
        "\n",
        "    pickle.dump( link_list, open( filename, \"wb\" ) )\n",
        "\n",
        "\n",
        "\n",
        "#load object from pickle file\n",
        "def load_obj(name):\n",
        "    file = open(name,'rb')\n",
        "    object_file = pickle.load(file)\n",
        "    file.close()\n",
        "\n",
        "    return object_file\n",
        "\n",
        "\n",
        "# save object in pickle\n",
        "def load_obj_no_sort(name):\n",
        "    file = open(name,'rb')\n",
        "    object_file = pickle.load(file)\n",
        "    file.close()\n",
        "\n",
        "    return object_file\n",
        "\n",
        "\n",
        "#load last visit queue for resume of crawling\n",
        "def load_obj_no_sort_w(filename):\n",
        "    global link_queue\n",
        "    link_list =[]\n",
        "\n",
        "    if (os.path.isfile(filename)):\n",
        "        file = open(filename,'rb')\n",
        "        link_list = pickle.load(file)\n",
        "        file.close()\n",
        "\n",
        "        for link in link_list:\n",
        "            link_queue.put(link)\n",
        "        return link_queue\n",
        "\n",
        "    else:\n",
        "        print(\"no file found\")\n",
        "        return"
      ],
      "metadata": {
        "id": "eMKDST0G02mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reset all variables\n",
        "def reset_global_variables():\n",
        "    global stopwords\n",
        "    global term_doc_freq_vector\n",
        "    global doc_term_freq_vector\n",
        "    global page_doc_map\n",
        "    global doc_page_map\n",
        "    global page_ref_count\n",
        "    global doc_count\n",
        "    global link_queue\n",
        "    global last_doc_index\n",
        "    global page_queued_map\n",
        "    global start_time\n",
        "\n",
        "    stopwords = {}\n",
        "    term_doc_freq_vector = {}\n",
        "    doc_term_freq_vector = {}\n",
        "    page_doc_map = {}\n",
        "    doc_page_map = {}\n",
        "    page_ref_count = {}\n",
        "    doc_count = 0\n",
        "    link_queue = queue.Queue()\n",
        "    last_doc_index = -1\n",
        "    page_queued_map = {}\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "#delete all files\n",
        "def delete_all_files():\n",
        "    try:\n",
        "        delete_directories(list_dir)\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        delete_file(\"doc_count.p\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        delete_file(\"doc_term_freq_vector.p\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        delete_file(\"doc_term_freq_vector_norm.p\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        delete_file(\"doc_url_map.p\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        delete_file(\"link_queue.p\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        delete_file(\"page_ref_count.p\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        delete_file(\"term_doc_freq_vector.p\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        delete_file(\"url_doc_map.p\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# save crawled doc object\n",
        "def save_all_obj():\n",
        "    global page_doc_map\n",
        "    global doc_page_map\n",
        "    global page_ref_count\n",
        "    global doc_count\n",
        "    global link_queue\n",
        "\n",
        "    save_obj(page_doc_map, \"url_doc_map\", \"value\", \"auto\")\n",
        "    save_obj(doc_page_map, \"doc_url_map\", \"key\", \"auto\")\n",
        "    save_obj_no_sort(doc_count, \"doc_count\")\n",
        "    save_obj_no_sort_w(link_queue, \"link_queue\")\n",
        "    save_obj(page_ref_count, \"page_ref_count\", \"value\", \"reverse\")\n",
        "    print(\"saved objects\")\n",
        "\n",
        "#save tfidf\n",
        "def save_all_obj_tfidf(doc_term_freq_vector_norm_new):\n",
        "    global term_doc_freq_vector\n",
        "    global doc_term_freq_vector\n",
        "\n",
        "    save_obj_without_sort(term_doc_freq_vector, \"term_doc_freq_vector\")\n",
        "    save_obj_without_sort(doc_term_freq_vector, \"doc_term_freq_vector\")\n",
        "    save_obj_without_sort(doc_term_freq_vector_norm_new, \"doc_term_freq_vector_norm\")\n",
        "\n",
        "\n",
        "\n",
        "# load crawled doc object\n",
        "def load_all_obj():\n",
        "    global page_doc_map\n",
        "    global doc_page_map\n",
        "    global page_ref_count\n",
        "    global doc_count\n",
        "    global link_queue\n",
        "\n",
        "    page_doc_map = load_obj(\"url_doc_map.p\")\n",
        "    doc_page_map = load_obj(\"doc_url_map.p\")\n",
        "    #doc_count = load_obj_no_sort(\"doc_count.p\")\n",
        "    doc_count = max(doc_page_map.keys())\n",
        "    link_queue = load_obj_no_sort_w(\"link_queue.p\")\n",
        "    page_ref_count = load_obj(\"page_ref_count.p\")\n",
        "    print(\"loaded objects\")\n",
        "\n",
        "\n",
        "def load_obj_search():\n",
        "    global total_number_docs\n",
        "    global doc_url_map\n",
        "    global term_doc_freq_vector\n",
        "    global doc_term_freq_vector\n",
        "    global doc_term_freq_vector_norm\n",
        "\n",
        "    stopword_path = \"english.stopwords.txt\"\n",
        "    doc_url_map_file = \"doc_url_map.p\"\n",
        "    term_doc_freq_file = \"term_doc_freq_vector.p\"\n",
        "    doc_term_freq_file = \"doc_term_freq_vector.p\"\n",
        "    doc_term_freq_file_norm = \"doc_term_freq_vector_norm.p\"\n",
        "    doc_count = \"doc_count.p\"\n",
        "\n",
        "    total_number_docs = load_obj(doc_count)\n",
        "    doc_url_map = load_obj(doc_url_map_file)\n",
        "    term_doc_freq_vector = load_obj(term_doc_freq_file)\n",
        "    doc_term_freq_vector = load_obj(doc_term_freq_file)\n",
        "    doc_term_freq_vector_norm = load_obj(doc_term_freq_file_norm)\n",
        "    load_stopwords(stopword_path)\n",
        "\n",
        "#print elapsed time in hh:mm:ss format\n",
        "def format_time(start_time, end_time):\n",
        "    elsapsed_time = end_time - start_time\n",
        "    hr = int(elsapsed_time)//3600\n",
        "    min_ = (int(elsapsed_time) - (hr * 3600))/60\n",
        "    sec = int(elsapsed_time) - hr * 3600 - min_ * 60\n",
        "    print(\"HH:Min:Sec > \" + str(hr) +\" hr \" + str(min_) + \" min \"+ str(sec) + \"sec\")"
      ],
      "metadata": {
        "id": "94AQrpkb1Nln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5tPYcg80CZ0"
      },
      "outputs": [],
      "source": [
        "# URLS and Text preprocessing functions\n",
        "\n",
        "# remove fragment identifier # and repeated loop url for php and asp\n",
        "def remove_url_frag_id(url):\n",
        "    if \".php\" in url:\n",
        "        url = url.split('.php')\n",
        "        if(len(url)>1):\n",
        "            url =url[0] + \".php\"\n",
        "    elif \".aspx\" in url:\n",
        "        url = url.split('.aspx')\n",
        "        if(len(url)>1):\n",
        "            url =url[0] + \".aspx\"\n",
        "    url = url.split('#')[0]\n",
        "\n",
        "    return url\n",
        "\n",
        "\n",
        "# remove fragment identifier #\n",
        "def remove_url_frag_simple(url):\n",
        "    url = url.split('#')[0]\n",
        "    return url\n",
        "\n",
        "\n",
        "# removes \"/\" from url\n",
        "def remove_slash_before_or_after(url, type_r):\n",
        "    if(type_r == \"before\"):\n",
        "        if url.startswith(\"/\"):\n",
        "            url = url[1:]\n",
        "        return url\n",
        "\n",
        "    elif(type_r == \"after\"):\n",
        "        if url[-1]==\"/\":\n",
        "            url = url.rsplit('/', 1)[0]\n",
        "        return url\n",
        "\n",
        "\n",
        "# remove http or https from webpage urls\n",
        "def strip_http_s(url):\n",
        "    url = url.replace(\"https://\",\"\")\n",
        "    url = url.replace(\"http://\",\"\")\n",
        "    url = url.rstrip('\\/')\n",
        "    url = \"http://\"+ url\n",
        "\n",
        "    return url\n",
        "\n",
        "\n",
        "# check if url is in selected domain name\n",
        "def check_if_in_domain(url, domain):\n",
        "    if(domain in url):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# file type extention type exclusion\n",
        "def is_excluded_type(extension):\n",
        "    exclude_list = [\"jpg\", \"jpeg\", \"png\", \"mp3\", \"mp4\", \"xlx\"]\n",
        "    if extension in exclude_list:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "#check whether URL is valid\n",
        "def check_valid_URL(url):\n",
        "    url_reg = re.compile(\n",
        "        r'^(?:http|ftp)s?://' # http:// or https://\n",
        "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
        "        r'localhost|' #localhost...\n",
        "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
        "        r'(?::\\d+)?' # optional port\n",
        "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
        "\n",
        "    is_valid = url_reg.match(url)\n",
        "\n",
        "    return is_valid\n",
        "\n",
        "\n",
        "#get extention of link to check the link type(.txt, .pdf, or html)\n",
        "def get_page_extention(url):\n",
        "    weblink_extention = url.rsplit('.', 1)[-1]\n",
        "    return weblink_extention\n",
        "\n",
        "\n",
        "#remove hyperlink from web page text for preprocessing\n",
        "def remove_hyper_link(text):\n",
        "    URLless_string = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', text)\n",
        "    return URLless_string\n",
        "\n",
        "\n",
        "#remove special character in single line\n",
        "def remove_special_char(line):\n",
        "    line = re.sub('[^a-zA-Z]+', ' ', line)\n",
        "    return line\n",
        "\n",
        "\n",
        "# get all the url/links to other pages from current pages\n",
        "def get_all_links(url, html):\n",
        "    global domain\n",
        "    global link_queue\n",
        "    global page_queued_map\n",
        "\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    links = soup.find_all('a')\n",
        "\n",
        "\n",
        "    for tag in links:\n",
        "        link = tag.get('href', None)\n",
        "\n",
        "        if link is not None:\n",
        "            try:\n",
        "                link_extention = get_page_extention(link)\n",
        "\n",
        "                if(link == \"\" or link == \"#\" or link_extention == \"ppt\" or is_excluded_type(link_extention) == 1):\n",
        "                    a=1\n",
        "\n",
        "                elif(link_extention in [\"pdf\", \"docx\", \"pptx\", \"txt\"]):\n",
        "                    if(check_valid_URL(link)):\n",
        "                        link_original = strip_http_s(link)\n",
        "\n",
        "                        if link_original not in page_queued_map:\n",
        "                            page_queued_map[link_original] = 1\n",
        "                            link_queue.put(link)\n",
        "\n",
        "                    else:\n",
        "                        modified_url = remove_url_frag_id(url)\n",
        "                        modified_url = remove_slash_before_or_after(modified_url, \"after\")\n",
        "                        modified_link = remove_slash_before_or_after(link, \"before\")\n",
        "                        modified_link = modified_url + \"/\" + modified_link\n",
        "\n",
        "                        if(check_valid_URL(modified_link)):\n",
        "                            link_original = strip_http_s(modified_link)\n",
        "\n",
        "                            if link_original not in page_queued_map:\n",
        "                                page_queued_map[link_original] = 1\n",
        "                                link_queue.put(modified_link)\n",
        "\n",
        "                else:\n",
        "                    is_valid = check_valid_URL(link)\n",
        "                    if(is_valid):\n",
        "                        modified_link = remove_url_frag_id(link)\n",
        "                        modified_link = remove_slash_before_or_after(modified_link, \"after\")\n",
        "                        link_original = strip_http_s(modified_link)\n",
        "\n",
        "                        if(check_if_in_domain(modified_link, domain) == 1):\n",
        "\n",
        "                            if link_original not in page_queued_map:\n",
        "                                page_queued_map[link_original] = 1\n",
        "                                link_queue.put(modified_link)\n",
        "\n",
        "                    else:\n",
        "                        modified_url= remove_url_frag_id(url)\n",
        "                        modified_url = remove_slash_before_or_after(modified_url, \"after\")\n",
        "                        modified_link = remove_slash_before_or_after(url, \"before\")\n",
        "\n",
        "                        if(modified_url!=modified_link):\n",
        "\n",
        "                            if  modified_url not in modified_link:\n",
        "                                modified_link = modified_url + \"/\" + modified_link\n",
        "\n",
        "                                if(check_if_in_domain(modified_link, domain) == 1):\n",
        "                                    if(check_valid_URL(modified_link)):\n",
        "                                        link_original = strip_http_s(modified_link)\n",
        "\n",
        "                                        if link_original not in page_queued_map:\n",
        "                                            page_queued_map[link_original] = 1\n",
        "                                            link_queue.put(modified_link)\n",
        "                            else:\n",
        "                                if(check_if_in_domain(modified_link, domain) == 1):\n",
        "                                    if(check_valid_URL(modified_link)):\n",
        "                                        link_original = strip_http_s(modified_link)\n",
        "\n",
        "                                        if link_original not in page_queued_map:\n",
        "                                            page_queued_map[link_original] = 1\n",
        "                                            link_queue.put(modified_link)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "\n",
        "# In[180]:\n",
        "\n",
        "\n",
        "\n",
        "# convert pdf to text using \"pdf2text\"\n",
        "def pdf_to_text(input_pdf, file_name):\n",
        "    global crawled_web_dir\n",
        "    os.system((\"pdftotext %s %s\") %( input_pdf, crawled_web_dir+\"//\"+file_name))\n",
        "\n",
        "\n",
        "# convert pptx to text\n",
        "def pptx_to_text(book_path):\n",
        "    prs = Presentation(book_path)\n",
        "    text = \"\"\n",
        "\n",
        "    for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if not shape.has_text_frame:\n",
        "                continue\n",
        "            for paragraph in shape.text_frame.paragraphs:\n",
        "                for run in paragraph.runs:\n",
        "                    text = text_runs + run.text\n",
        "    return text\n",
        "\n",
        "\n",
        "#import pdf, docx, pptx from url/single web link and convert to text and save in directory\n",
        "def import_convert_preprocess(url, extension):\n",
        "    global doc_count\n",
        "    global crawled_web_dir_preprocessed\n",
        "    global crawled_web_dir\n",
        "    global crawled_web_dir_conv_need\n",
        "    global page_doc_map\n",
        "    url_map_name = url\n",
        "\n",
        "    if(url_map_name not in page_doc_map):\n",
        "        page_doc_map[url_map_name] = -1\n",
        "        page_ref_count[url_map_name] = 1\n",
        "\n",
        "        try:\n",
        "            doc_count_temp = doc_count + 1\n",
        "            book_name = \"\"\n",
        "            if extension == \"pdf\":\n",
        "                book_name = str(doc_count_temp) + \".pdf\"\n",
        "            elif extension == \"docx\":\n",
        "                book_name = str(doc_count_temp) + \".docx\"\n",
        "            elif extension == \"pptx\":\n",
        "                book_name = str(doc_count_temp) + \".pptx\"\n",
        "\n",
        "            book_path = crawled_web_dir_conv_need + \"\\\\\" + book_name\n",
        "\n",
        "            a = requests.get(url, stream=True)\n",
        "\n",
        "            with open(book_path, 'wb') as book:\n",
        "                for block in a.iter_content(512):\n",
        "                    if not block:\n",
        "                        break\n",
        "                    book.write(block)\n",
        "\n",
        "            book.close()\n",
        "\n",
        "            file_name = str(doc_count_temp)+\".txt\"\n",
        "            file_path = crawled_web_dir+ \"\\\\\" + file_name\n",
        "            is_valid_for_indexing = 555\n",
        "            if extension == \"pdf\":\n",
        "                pdf_to_text(book_path, file_name)\n",
        "                is_valid_for_indexing = preprocess_one_doc_from_pdf(crawled_web_dir, file_name, crawled_web_dir_preprocessed)\n",
        "\n",
        "            elif extension == \"docx\":\n",
        "                text = docx2txt.process(book_path)\n",
        "                save_text(text, crawled_web_dir, file_name)\n",
        "                is_valid_for_indexing = preprocess_one_doc(crawled_web_dir, file_name, crawled_web_dir_preprocessed)\n",
        "\n",
        "            elif extension == \"pptx\":\n",
        "                text = pptx_to_text(book_path)\n",
        "                save_text(text, crawled_web_dir, file_name)\n",
        "                is_valid_for_indexing = preprocess_one_doc(crawled_web_dir, file_name, crawled_web_dir_preprocessed)\n",
        "\n",
        "\n",
        "            if(is_valid_for_indexing == 1) :\n",
        "                doc_count = doc_count + 1\n",
        "                page_doc_map[url_map_name] = doc_count\n",
        "                doc_page_map[doc_count] = url_map_name\n",
        "                page_ref_count[url_map_name] = 1\n",
        "            else:\n",
        "                delete_file(book_path)\n",
        "                delete_file(file_path)\n",
        "                page_doc_map[url_map_name] = -2\n",
        "\n",
        "        except IOError:\n",
        "            page_doc_map[url_map_name]= -1\n",
        "    else:\n",
        "        page_ref_count[url_map_name] = page_ref_count[url_map_name] + 1\n",
        "\n",
        "\n",
        "\n",
        "#remove all html and scripting\n",
        "def remove_extra_space(txt):\n",
        "    # Removes all blank lines\n",
        "    txt = re.sub(r'\\n\\s*\\n', '\\n', txt)\n",
        "    return txt\n",
        "\n",
        "\n",
        "# clean html of the tag and markups\n",
        "def clean_html(html_text):\n",
        "    global crawled_web_dir\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "\n",
        "    for script in soup(['style', 'script', 'head', 'title', 'meta', '[document]']):\n",
        "        script.extract()\n",
        "    for tag in soup.find_all('a'):\n",
        "        tag.replaceWith('')\n",
        "    for tag in soup.find_all('footer'):\n",
        "        tag.replaceWith('')\n",
        "\n",
        "    clean_text = soup.get_text()\n",
        "    clean_text = remove_extra_space(clean_text)\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import text from single web page and preprocess\n",
        "def fetch_extract_html_txt(url):\n",
        "    global doc_count\n",
        "    global domain\n",
        "    global crawled_web_dir\n",
        "    global crawled_web_dir_preprocessed\n",
        "\n",
        "    if(check_if_in_domain(url, domain) == 0):\n",
        "        return\n",
        "    url_map_name = url\n",
        "\n",
        "    if(url_map_name in page_doc_map):\n",
        "        page_ref_count[url_map_name] = page_ref_count[url_map_name] + 1\n",
        "\n",
        "    else:\n",
        "        page_doc_map[url_map_name] = -1\n",
        "        page_ref_count[url_map_name] = 1\n",
        "\n",
        "        try:\n",
        "            html = urllib.request.urlopen(url)\n",
        "            html_text = html.read()\n",
        "\n",
        "            if(html_text.strip() == \"\"):\n",
        "                return\n",
        "\n",
        "            clean_text = clean_html(html_text)\n",
        "            clean_text = clean_text.strip()\n",
        "\n",
        "            if clean_text.strip()==\"\":\n",
        "                return\n",
        "\n",
        "            doc_count = doc_count + 1\n",
        "            page_doc_map[url_map_name] = doc_count\n",
        "            doc_page_map[doc_count] = url_map_name\n",
        "            save_text(clean_text, crawled_web_dir, str(doc_count)+\".txt\")\n",
        "\n",
        "            file_name = str(doc_count)+\".txt\"\n",
        "            file_path = crawled_web_dir+ \"\\\\\" + file_name\n",
        "\n",
        "            is_valid_for_indexing = preprocess_one_doc(crawled_web_dir, file_name, crawled_web_dir_preprocessed)\n",
        "\n",
        "            if(is_valid_for_indexing == 0) :\n",
        "                delete_file(file_path)\n",
        "                page_doc_map[url_map_name] = -2\n",
        "                doc_count = doc_count - 1\n",
        "\n",
        "            get_all_links(url, html_text)\n",
        "\n",
        "        except:\n",
        "            page_doc_map[url_map_name]= -1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[181]:\n",
        "\n",
        "\n",
        "# preprocess files in a folder to remove punctuations, digits, special characters, url/web links\n",
        "# removes stop words given in file\n",
        "# convert to origin word/ do stemming\n",
        "\n",
        "\n",
        "# preprocess webpage text, docx, pptx text and save in directory\n",
        "def preprocess_one_doc(input_dir, input_filename, output_dir):\n",
        "    ps = PorterStemmer()\n",
        "    input_file_path = input_dir + \"\\\\\"+ input_filename\n",
        "    text = \"\"\n",
        "    count = 0\n",
        "\n",
        "    try:\n",
        "        with open(input_file_path, 'r') as content_file:\n",
        "            for line in content_file:\n",
        "                if(line in ['\\n', '\\r\\n','\\r']):\n",
        "                    continue\n",
        "\n",
        "                line = line.strip()\n",
        "                line = remove_hyper_link(line)\n",
        "                line = remove_special_char(line)\n",
        "                line = line.lower()\n",
        "                line = re.sub(' +',' ',line)\n",
        "                words = line.split(\" \")\n",
        "\n",
        "                for word in words:\n",
        "                    word = word.strip()\n",
        "                    word = remove_special_char(word)\n",
        "                    word = re.sub(' +','',word)\n",
        "\n",
        "                    if word not in stopwords and word != \" \" and word != \"\":\n",
        "                        stem_word = ps.stem(word)\n",
        "                        text = text + \" \" + stem_word\n",
        "                        count = count + 1\n",
        "\n",
        "            if(count > 50):\n",
        "                save_text(text, output_dir, input_filename)\n",
        "                return 1\n",
        "\n",
        "            else:\n",
        "\n",
        "                return 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# preprocess pdf text and  save in directory\n",
        "def preprocess_one_doc_from_pdf(input_dir, input_filename, output_dir):\n",
        "    ps = PorterStemmer()\n",
        "    input_file_path = input_dir + \"\\\\\"+ input_filename\n",
        "    text = \"\"\n",
        "    count = 0\n",
        "    ret_val = 999\n",
        "\n",
        "    try:\n",
        "        with open(input_file_path, 'rb') as content_file:\n",
        "            for line in content_file:\n",
        "                line = line.decode(\"utf-8\")\n",
        "                if(line in ['\\n', '\\r\\n','\\r']):\n",
        "                    continue\n",
        "\n",
        "                line = line.strip()\n",
        "                line = remove_hyper_link(line)\n",
        "                line = remove_special_char(line)\n",
        "                line = line.lower()\n",
        "                line = re.sub(' +',' ',line)\n",
        "                words = line.split(\" \")\n",
        "\n",
        "                for word in words:\n",
        "                    word = word.strip()\n",
        "                    word = remove_special_char(word)\n",
        "                    word = re.sub(' +','',word)\n",
        "\n",
        "                    if word not in stopwords and word != \" \" and word != \"\":\n",
        "                        stem_word = ps.stem(word)\n",
        "                        text = text + \" \" + stem_word\n",
        "                        count = count + 1\n",
        "\n",
        "            if(count > 50):\n",
        "                save_text(text, output_dir, input_filename)\n",
        "                return 1\n",
        "\n",
        "            else:\n",
        "                return  0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "# In[182]:\n",
        "\n",
        "\n",
        "\n",
        "# crawl through single webpage\n",
        "def webpage_crawler(total_number_docs):\n",
        "    global doc_count\n",
        "    global link_queue\n",
        "    global last_doc_index\n",
        "    global start_time\n",
        "    global crawled_web_dir\n",
        "    global crawled_web_dir_conv_need\n",
        "\n",
        "    if(doc_count % 100 == 0 and last_doc_index != doc_count):\n",
        "        print(\"Extracted Documents: \" + str(doc_count))\n",
        "        last_doc_index = doc_count\n",
        "\n",
        "    if(doc_count % 200 == 0):\n",
        "        format_time(start_time, time.time())\n",
        "\n",
        "    url = link_queue.get()\n",
        "    #print(doc_count+1, \" : \", url)\n",
        "\n",
        "    try:\n",
        "        link_extention = get_page_extention(url)\n",
        "\n",
        "        if(url == \"\" or link_extention == \"ppt\"):\n",
        "            a=1\n",
        "        elif(link_extention in [\"pdf\",\"docx\", \"pptx\"]):\n",
        "            import_convert_preprocess(url, link_extention)\n",
        "        elif(link_extention == \"txt\"):\n",
        "            fetch_extract_html_txt(url)\n",
        "        else:\n",
        "            fetch_extract_html_txt(url)\n",
        "\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "# crawl through a website\n",
        "def website_crawler(total_number_docs):\n",
        "    global link_queue\n",
        "    global doc_count\n",
        "    global total_number_doc\n",
        "\n",
        "    while(doc_count < total_number_docs):\n",
        "        if(link_queue.empty()):\n",
        "            print(\"Queue is empty\")\n",
        "            return\n",
        "\n",
        "        if(doc_count%200 == 0 and doc_count != 0):\n",
        "            save_all_obj()\n",
        "\n",
        "        webpage_crawler(total_number_docs)\n",
        "\n",
        "    save_all_obj()\n",
        "\n",
        "\n",
        "# In[183]:\n",
        "\n",
        "\n",
        "\n",
        "#web crawler main\n",
        "def web_crawling_main(url, domain, total_page_count):\n",
        "    print(\"Start Time: \", datetime.datetime.time(datetime.datetime.now()))\n",
        "\n",
        "    delete_all_files()\n",
        "    reset_global_variables()\n",
        "\n",
        "    create_directories(list_dir)\n",
        "\n",
        "    load_stopwords(stopword_path)\n",
        "\n",
        "    url = remove_slash_before_or_after(url, \"after\")\n",
        "    link_original = strip_http_s(url)\n",
        "    page_queued_map[link_original] = 1\n",
        "    print(url)\n",
        "    link_queue.put(url)\n",
        "\n",
        "    total_number_docs = total_page_count\n",
        "\n",
        "    website_crawler(total_number_docs)\n",
        "\n",
        "\n",
        "# Update crawling from last saved url location\n",
        "def web_crawling_main_update(url, domain, num_add_doc):\n",
        "    global page_doc_map\n",
        "    global doc_page_map\n",
        "    global page_ref_count\n",
        "    global total_number_docs\n",
        "    global doc_count\n",
        "\n",
        "    print(\"Start Time: \", datetime.datetime.time(datetime.datetime.now()))\n",
        "\n",
        "    reset_global_variables()\n",
        "    load_stopwords(stopword_path)\n",
        "    load_all_obj()\n",
        "\n",
        "    total_number_docs = doc_count + num_add_doc\n",
        "    website_crawler(total_number_docs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[184]:\n",
        "\n",
        "\n",
        "def cosine_similarity(v1,v2):\n",
        "    sumxx, sumxy, sumyy = 0, 0, 0\n",
        "    for i in range(len(v1)):\n",
        "        x = v1[i];\n",
        "        y = v2[i]\n",
        "        sumxx += x*x\n",
        "        sumyy += y*y\n",
        "        sumxy += x*y\n",
        "    return sumxy/math.sqrt(sumxx*sumyy)\n",
        "\n",
        "\n",
        "# In[185]:\n",
        "\n",
        "\n",
        "\n",
        "#build inverted index for all files present in preprocessed file directory\n",
        "def inverse_document_indexer(preprocessed_file_dir_path):\n",
        "    dirs = os.listdir(preprocessed_file_dir_path)\n",
        "    i = 0\n",
        "\n",
        "    for file in dirs:\n",
        "        filepath = preprocessed_file_dir_path + \"\\\\\"+ file\n",
        "        text = \"\"\n",
        "        i = i + 1\n",
        "\n",
        "        if(i % 1000 == 0):\n",
        "            print(\"Building inverse document index for file no: \"+str(i))\n",
        "            print(\"Current Time: \", datetime.datetime.time(datetime.datetime.now()))\n",
        "\n",
        "        try:\n",
        "            with open(filepath, 'r') as content_file:\n",
        "                file_name = str(file)[:-4]\n",
        "\n",
        "                doc_term_freq_vector[file_name] = {}\n",
        "                single_doc_term_freq_vector = doc_term_freq_vector[file_name]\n",
        "\n",
        "                for line in content_file:\n",
        "                    line = line.strip()\n",
        "                    words = line.split(\" \")\n",
        "\n",
        "                    for word in words:\n",
        "                        word = word.strip()\n",
        "\n",
        "                        if word != \"\":\n",
        "                            if word not in term_doc_freq_vector:\n",
        "                                single_term_doc_freq_vector = {}\n",
        "                                single_term_doc_freq_vector[file_name] = 1\n",
        "                                single_term_doc_freq_vector[\"DocFreq\"] = 1\n",
        "                                term_doc_freq_vector[word] = single_term_doc_freq_vector\n",
        "\n",
        "                            else:\n",
        "\n",
        "                                single_term_doc_freq_vector = term_doc_freq_vector[word]\n",
        "\n",
        "                                if file_name not in single_term_doc_freq_vector:\n",
        "                                    single_term_doc_freq_vector[file_name] = 1\n",
        "                                    single_term_doc_freq_vector[\"DocFreq\"] = single_term_doc_freq_vector[\"DocFreq\"] + 1\n",
        "                                    term_doc_freq_vector[word] = single_term_doc_freq_vector\n",
        "\n",
        "                                else:\n",
        "                                    single_term_doc_freq_vector[file_name] = single_term_doc_freq_vector[file_name] + 1\n",
        "                                    term_doc_freq_vector[word] = single_term_doc_freq_vector\n",
        "\n",
        "                            a=1\n",
        "                            if \"DocMaxFreq\" not in single_doc_term_freq_vector:\n",
        "                                single_doc_term_freq_vector[\"DocMaxFreq\"] = 1\n",
        "\n",
        "                            if word not in single_doc_term_freq_vector:\n",
        "                                single_doc_term_freq_vector[word] = 1\n",
        "                                doc_term_freq_vector[file_name] = single_doc_term_freq_vector\n",
        "\n",
        "                            else:\n",
        "                                single_doc_term_freq_vector[word] = single_doc_term_freq_vector[word] + 1\n",
        "\n",
        "                                if(single_doc_term_freq_vector[word] > single_doc_term_freq_vector[\"DocMaxFreq\"]):\n",
        "                                    single_doc_term_freq_vector[\"DocMaxFreq\"] = single_doc_term_freq_vector[word]\n",
        "                                doc_term_freq_vector[file_name] = single_doc_term_freq_vector\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "#builds tfidf from inverse document index\n",
        "def tfidf_document_text(term_doc_freq_vector, doc_term_freq_vector):\n",
        "    global total_number_docs\n",
        "\n",
        "    total_number_docs = load_obj(\"doc_count.p\")\n",
        "\n",
        "    doc_term_freq_vector_normalized = doc_term_freq_vector\n",
        "\n",
        "\n",
        "    for doc in doc_term_freq_vector_normalized:\n",
        "        for term in doc_term_freq_vector_normalized[doc]:\n",
        "\n",
        "            if(term != \"DocMaxFreq\"):\n",
        "                doc_freq = term_doc_freq_vector[term][\"DocFreq\"]\n",
        "                doc_term_freq_vector_normalized[doc][term] = (doc_term_freq_vector[doc][term]/doc_term_freq_vector[doc][\"DocMaxFreq\"])*(math.log2(total_number_docs/doc_freq))\n",
        "\n",
        "\n",
        "    for doc in doc_term_freq_vector_normalized:\n",
        "        del doc_term_freq_vector_normalized[doc][\"DocMaxFreq\"]\n",
        "\n",
        "    return doc_term_freq_vector_normalized\n",
        "\n",
        "\n",
        "\n",
        "def inverse_document_indexer_final(crawled_web_dir_preprocessed, stopword_path):\n",
        "    inverse_document_indexer(crawled_web_dir_preprocessed)\n",
        "    doc_term_freq_vector_norm = tfidf_document_text(term_doc_freq_vector, doc_term_freq_vector)\n",
        "    save_all_obj_tfidf(doc_term_freq_vector_norm)\n",
        "\n",
        "    return term_doc_freq_vector, doc_term_freq_vector, doc_term_freq_vector_norm\n",
        "\n",
        "\n",
        "# In[186]:\n",
        "\n",
        "\n",
        "#query preprocessing\n",
        "def query_preprocessor(query_str):\n",
        "    ps = PorterStemmer()\n",
        "    query_dict = {}\n",
        "\n",
        "    query_str_modified = query_str.strip()\n",
        "    query_str_modified = remove_special_char(query_str_modified)\n",
        "    query_str_modified = query_str_modified.lower()\n",
        "    query_str_modified = re.sub(' +',' ',query_str_modified)\n",
        "    words = query_str_modified.split(\" \")\n",
        "\n",
        "    max_freq = 0\n",
        "    N = 0\n",
        "\n",
        "    for word in words:\n",
        "        word = word.strip()\n",
        "\n",
        "        if word not in stopwords and word !=\"\":\n",
        "            word = ps.stem(word)\n",
        "\n",
        "            if word not in query_dict:\n",
        "                query_dict[word] = 1\n",
        "            else:\n",
        "                query_dict[word] = query_dict[word] + 1\n",
        "\n",
        "            if(query_dict[word] > max_freq):\n",
        "                max_freq = query_dict[word]\n",
        "\n",
        "            N +=1\n",
        "\n",
        "    return query_dict, max_freq, N\n",
        "\n",
        "\n",
        "#generate normalized term vector for query\n",
        "def query_normalizer(query_dict, max_freq, total_number_docs, term_doc_freq_vector):\n",
        "    query_dict_normalized = {}\n",
        "    doc_term_freq_vector_normalized = doc_term_freq_vector\n",
        "\n",
        "    for word in query_dict:\n",
        "        if word in term_doc_freq_vector:\n",
        "            query_dict_normalized[word] =  ( 0.5  +  (0.5 * query_dict[word] / max_freq) ) * (math.log2((total_number_docs+1)/(term_doc_freq_vector[word][\"DocFreq\"]+1)))\n",
        "        else:\n",
        "            query_dict_normalized[word] =  ( 0.5  +  (0.5 * query_dict[word] / max_freq) ) * (math.log2((total_number_docs+1)))\n",
        "\n",
        "    return query_dict_normalized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# retrieve documents for query\n",
        "def retrieve_docs_with_query_word(query_term_freq_vect_norm, term_doc_freq_vector):\n",
        "    docs_with_query_terms = []\n",
        "\n",
        "    for word in query_term_freq_vect_norm:\n",
        "        if word in term_doc_freq_vector:\n",
        "            docs = term_doc_freq_vector[word]\n",
        "            for doc in docs:\n",
        "                if(doc != \"DocFreq\"):\n",
        "                    docs_with_query_terms.append(doc)\n",
        "\n",
        "    return docs_with_query_terms\n",
        "\n",
        "# retrieve relevant document\n",
        "def calculate_cosine_query_doc(docs_with_query_terms, query_term_freq_norm, term_doc_freq_vector, doc_term_freq_vector):\n",
        "    cosine_query_doc = {}\n",
        "\n",
        "    for doc in docs_with_query_terms:\n",
        "        temp = {}\n",
        "\n",
        "        for word in doc_term_freq_vector_norm[doc]:\n",
        "            if word in query_term_freq_norm:\n",
        "                temp[word] = query_term_freq_norm[word]\n",
        "            else:\n",
        "                temp[word] = 0\n",
        "\n",
        "        doc_v=[]\n",
        "        query_v = []\n",
        "\n",
        "        for word in doc_term_freq_vector_norm[doc]:\n",
        "            doc_v.append(doc_term_freq_vector_norm[doc][word])\n",
        "            query_v.append(temp[word])\n",
        "        cosine_query_doc[doc] = cosine_similarity(doc_v, query_v)\n",
        "\n",
        "    return cosine_query_doc\n",
        "\n",
        "\n",
        "#get url from file namedoc\n",
        "def get_url(cosine_query_doc, doc_url_map):\n",
        "    url_list = []\n",
        "    similarity = []\n",
        "    doc_list = []\n",
        "    similarity_map = {}\n",
        "\n",
        "    cosine_query_doc_new = sorted(cosine_query_doc.items(), key=operator.itemgetter(1), reverse = True)\n",
        "    cosine_query_doc_newest = {}\n",
        "\n",
        "    for doc in cosine_query_doc_new:\n",
        "        cosine_query_doc_newest[doc[0]] = doc[1]\n",
        "\n",
        "    for doc in cosine_query_doc_newest:\n",
        "        url_list.append(doc_url_map[int(doc)])\n",
        "        similarity.append(cosine_query_doc_newest[doc])\n",
        "        similarity_map[doc] = cosine_query_doc_newest[doc]\n",
        "        doc_list.append(doc)\n",
        "\n",
        "    return url_list, doc_url_map, similarity, similarity_map, doc_list\n",
        "\n",
        "\n",
        "\n",
        "# In[187]:\n",
        "\n",
        "\n",
        "#Evaluation\n",
        "#finds relevant docs\n",
        "def relevant_doc(query_term_freq_vect, doc_term_freq_vector):\n",
        "    relevant_list = []\n",
        "    relevant_list_map = {}\n",
        "\n",
        "    for doc in doc_term_freq_vector:\n",
        "        doc_i = 0\n",
        "\n",
        "        for term in query_term_freq_vect:\n",
        "            if term not in doc_term_freq_vector[doc]:\n",
        "                doc_i = -1\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                doc_i = doc_i + 1\n",
        "\n",
        "        if(doc_i!=-1):\n",
        "            relevant_list.append(doc)\n",
        "            relevant_list_map[doc] = doc_i\n",
        "\n",
        "\n",
        "    return len(relevant_list), relevant_list, relevant_list_map\n",
        "\n",
        "\n",
        "#calculate number of relevant docs\n",
        "def num_relevant_doc_in_query(doc_list, query_term_freq_vect, doc_term_freq_vector):\n",
        "    relevant_list = []\n",
        "    relevant_list_map = {}\n",
        "\n",
        "    for doc in doc_list:\n",
        "        doc_i = 0\n",
        "\n",
        "        for term in query_term_freq_vect:\n",
        "            if term not in doc_term_freq_vector[str(doc)]:\n",
        "                doc_i = -1\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                doc_i = doc_i + 1\n",
        "\n",
        "        if(doc_i!=-1):\n",
        "            relevant_list.append(doc)\n",
        "            relevant_list_map[doc] = doc_i\n",
        "\n",
        "    return len(relevant_list), relevant_list, relevant_list_map\n",
        "\n",
        "\n",
        "# find precision, recall and f1 score\n",
        "def evaluation(num, relevant_list_len, qrelevant_list_len):\n",
        "    print(num, relevant_list_len, qrelevant_list_len)\n",
        "    recall = -999\n",
        "    precision = -999\n",
        "    f1 = -999\n",
        "\n",
        "    if relevant_list_len != 0:\n",
        "        recall = qrelevant_list_len/relevant_list_len\n",
        "\n",
        "    if num != 0:\n",
        "        precision = qrelevant_list_len/num\n",
        "\n",
        "    if recall != -999 or precision != -999:\n",
        "        if recall == -999:\n",
        "            recall = 0\n",
        "        elif precision == -999 :\n",
        "            precision = 0\n",
        "\n",
        "        f1= (2*precision*recall)/(precision + recall)\n",
        "    else:\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f1 = 0\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "def perfomance(query_str, num):\n",
        "    avg_precision = 0\n",
        "    avg_recall = 0\n",
        "    avg_f1 = 0\n",
        "    query_str_len = len(query_str)\n",
        "    i=1\n",
        "\n",
        "    for query in query_str:\n",
        "        url_list, doc_url_map, similarity, similarity_map, docs_with_query_terms, term_doc_freq_vector, query_term_freq_vect,doc_term_freq_vector,doc_list = web_search_main(query_str[query])\n",
        "        relevant_list_len, relevant_list, relevant_list_map = relevant_doc(query_term_freq_vect, doc_term_freq_vector)\n",
        "        qrelevant_list_len,relevant_list, qrelevant_list_map = num_relevant_doc_in_query(doc_list[:num], query_term_freq_vect, doc_term_freq_vector)\n",
        "\n",
        "        precision, recall, f1 = evaluation(num, relevant_list_len, qrelevant_list_len)\n",
        "\n",
        "        avg_precision = avg_precision + precision\n",
        "        avg_recall = avg_recall + recall\n",
        "        avg_f1 = avg_f1 + f1\n",
        "\n",
        "        print(i,\" : \", query_str[query], \" : precision : \", precision, \", recall : \", recall, \", f1 : \", f1)\n",
        "\n",
        "        i = i + 1\n",
        "\n",
        "\n",
        "    avg_precision = (avg_precision/query_str_len)\n",
        "    avg_recall = (avg_recall/query_str_len)\n",
        "    avg_f1 = (avg_f1/query_str_len)\n",
        "    print(\"Average precision : \", avg_precision)\n",
        "    print(\"Average recall : \", avg_recall)\n",
        "    print(\"Average f1 : \", avg_f1)\n",
        "\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main search function\n",
        "def web_search_main(query_str):\n",
        "    #time_1 = time.time()\n",
        "    global total_number_docs\n",
        "    global doc_url_map\n",
        "    global term_doc_freq_vector\n",
        "    global doc_term_freq_vector\n",
        "    global doc_term_freq_vector_norm\n",
        "\n",
        "    load_obj_search()\n",
        "\n",
        "    query_term_freq_vect, max_freq, N = query_preprocessor(query_str)\n",
        "    query_term_freq_vect_norm = query_normalizer(query_term_freq_vect, max_freq, total_number_docs, term_doc_freq_vector)\n",
        "    docs_with_query_terms = retrieve_docs_with_query_word(query_term_freq_vect_norm, term_doc_freq_vector)\n",
        "    cosine_query_doc = calculate_cosine_query_doc(docs_with_query_terms, query_term_freq_vect_norm, term_doc_freq_vector, doc_term_freq_vector)\n",
        "    url_list, doc_url_map, similarity, similarity_map, doc_list = get_url(cosine_query_doc, doc_url_map)\n",
        "    #format_time(time_1, time.time())\n",
        "\n",
        "    return url_list, doc_url_map, similarity, similarity_map, docs_with_query_terms, term_doc_freq_vector, query_term_freq_vect, doc_term_freq_vector,doc_list\n",
        "\n",
        "\n",
        "def search_engine_final_main(query_str, count):\n",
        "    i = 0\n",
        "    result = []\n",
        "    url_list, doc_url_map, similarity, similarity_map, docs_with_query_terms, term_doc_freq_vector, query_term_freq_vect, doc_term_freq_vector,doc_list = web_search_main(query_str)\n",
        "\n",
        "    for url in url_list:\n",
        "        url_row = []\n",
        "        print(i+1, \". \", url, \"\\nSimillarity: \", similarity[i])\n",
        "        url_row.append(str(i + 1)+\".\")\n",
        "        url_row.append(url)\n",
        "        url_row.append(similarity[i])\n",
        "        result.append(url_row)\n",
        "        i += 1\n",
        "\n",
        "        if i>count:\n",
        "            break\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# In[196]:\n",
        "\n",
        "\n",
        "#total_page_count = 9000\n",
        "\n",
        "#web_crawling_main(url, domain, total_page_count)\n",
        "\n",
        "\n",
        "# In[197]:\n",
        "\n",
        "\n",
        "\n",
        "#num_add_doc = 200\n",
        "#web_crawling_main_update(url, domain, num_add_doc)\n",
        "\n",
        "crawled_web_dir_preprocessed = \"web_text_preprocessed\"\n",
        "stopword_path = \"english.stopwords.txt\"\n",
        "#term_doc_freq_vector, doc_term_freq_vector, doc_term_freq_vector_norm = inverse_document_indexer_final(crawled_web_dir_preprocessed, stopword_path)\n",
        "\n",
        "query_str = {\"q1\":\"Data Science director\", \"q2\":\"President of the university\", \"q3\":\"Plagiarism \", \"q4\":\"Learner Data Institute\",\"q5\":\"International student office\",\n",
        "\"q6\":\"Music School admissions\", \"q7\":\"What is the mascot of the University of Memphis?\",\"q8\": \"College of Arts and Sciences Dean\", \"q9\":\"Fedex Institute of Technology \",\"q10\":\" to be or not to be\"}\n"
      ],
      "metadata": {
        "id": "adHxutSpIlqk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}